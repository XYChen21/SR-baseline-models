{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "un45ObH5EUwV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, PReLU, ZeroPadding2D, Add\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX4vm3h4EUwW",
        "outputId": "410f475b-86b4-41fd-fafb-8a687b98aff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75S-iaHOEUwY"
      },
      "source": [
        "### Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ooUZdWriEUwY"
      },
      "outputs": [],
      "source": [
        "TRAIN_DIR = '/content/drive/MyDrive/SRCNN/Train'\n",
        "TEST_DIR = '/content/drive/MyDrive/SRCNN/Test/Set5'\n",
        "VAL_DIR = '/content/drive/MyDrive/SRCNN/Test/Set14'\n",
        "SCALE = 3\n",
        "scales = [2,3,4]\n",
        "SIZE_INPUT = 33\n",
        "SIZE_LABEL = 33\n",
        "STRIDE = 14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY6oUEuwEUwZ"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gLohG3K2EUwZ"
      },
      "outputs": [],
      "source": [
        "def crop(image, factor):\n",
        "    # ensure no remainder while scaling\n",
        "\n",
        "    size = image.shape\n",
        "    size -= np.mod(size, factor)\n",
        "    image = image[:size[0], :size[1]]\n",
        "    return image\n",
        "\n",
        "def generator(data_path):\n",
        "    files = os.listdir(data_path)\n",
        "    def img_gen():\n",
        "        for file in files:\n",
        "            path = os.path.join(data_path, file)\n",
        "            img_arr = np.array(Image.open(path), dtype=np.uint8)\n",
        "            img_arr = np.array(cv2.cvtColor(np.array(img_arr), cv2.COLOR_RGB2YCrCb), dtype=float)\n",
        "            y_channel = img_arr[:,:,0]\n",
        "            for scale in scales:\n",
        "                img_label = tf.expand_dims(crop(y_channel, scale), axis=-1)\n",
        "                h, w, _ = img_label.shape\n",
        "\n",
        "                down_sample = tf.image.resize(img_label, (int(h/scale), int(w/scale)), method='bicubic')\n",
        "                img_input = tf.image.resize(down_sample, (h, w), method='bicubic')\n",
        "                \n",
        "                for x in range(0, h-SIZE_INPUT+1, STRIDE):\n",
        "                    for y in range(0, w-SIZE_INPUT+1, STRIDE):\n",
        "                        sub_img_input = img_input[x:x+SIZE_INPUT, y:y+SIZE_INPUT]\n",
        "                        sub_img_label = img_label[x:x+SIZE_INPUT, y:y+SIZE_INPUT]\n",
        "\n",
        "                        yield (sub_img_input, sub_img_label)\n",
        "    return img_gen\n",
        "\n",
        "def create_dataset(path):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator(path),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(SIZE_INPUT, SIZE_INPUT, 1),),\n",
        "            tf.TensorSpec(shape=(SIZE_LABEL, SIZE_LABEL, 1),)\n",
        "        )\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fpJAOttnEUwa"
      },
      "outputs": [],
      "source": [
        "train = create_dataset(TRAIN_DIR).shuffle(30000).batch(64)\n",
        "val = create_dataset(VAL_DIR).shuffle(1000).batch(64)\n",
        "# test = create_dataset(TEST_DIR).shuffle(1000).batch(256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BCw9FJz8EUwc"
      },
      "outputs": [],
      "source": [
        "def PSNR(y_true, y_pred):\n",
        "\tmax_pixel = 255.0\n",
        "\treturn 10.0 * np.log10((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true)))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek0c7mKBEUwb"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nQNKIwwOpDLh"
      },
      "outputs": [],
      "source": [
        "class VDSR(tf.keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Gradient clipping\n",
        "        lr = self.optimizer.lr\n",
        "        clip_value = 0.5/lr\n",
        "        clipped_grads = [tf.clip_by_value(grad, -clip_value, clip_value) for grad in gradients]\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(clipped_grads, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Stn3mjfxEUwc"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    input = Input(shape = (SIZE_INPUT, SIZE_INPUT, 1))\n",
        "    x = Conv2D(64, 3, kernel_initializer='he_normal', padding='same', activation='relu')(input)\n",
        "    for i in range(19):\n",
        "        x = Conv2D(64, 3, kernel_initializer='he_normal', padding='same', activation='relu')(x)\n",
        "    x = Conv2D(1, 3, kernel_initializer='he_normal', padding='same', activation='relu')(x)\n",
        "    output = Add()([input, x])\n",
        "    model = VDSR(inputs = input, outputs = output, name = \"VDSR\") \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.1),\n",
        "        loss=\"mse\",\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q11HnklEUwd",
        "outputId": "e9297913-79f8-422b-93db-59e65c79504e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VDSR\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 33, 33, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 33, 33, 64)   640         ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 33, 33, 64)   36928       ['conv2d_39[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 33, 33, 1)    577         ['conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 33, 33, 1)    0           ['input_2[0][0]',                \n",
            "                                                                  'conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 702,849\n",
            "Trainable params: 702,849\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "m = create_model()\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EqGu8b6AEUwd"
      },
      "outputs": [],
      "source": [
        "# save the model with min val_loss\n",
        "file_path = \"/content/drive/MyDrive/SRCNN/saved_model/vdsr.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function keeps the initial learning rate for the first ten epochs\n",
        "# and decreases it exponentially after that.\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 20:\n",
        "        return lr\n",
        "    elif epoch % 20 == 0:\n",
        "        return lr*0.1\n",
        "    return lr\n",
        "lr_decay = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "metadata": {
        "id": "RPC5fXuca2Ib"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv0-WWaWEUwd"
      },
      "outputs": [],
      "source": [
        "m.fit(\n",
        "    train,\n",
        "    epochs = 80,\n",
        "    validation_data = val,\n",
        "    callbacks = [\n",
        "        # EarlyStopping(monitor = 'val_loss', patience=5),\n",
        "        checkpoint,\n",
        "        # lr_decay\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H01IU0hEUwe"
      },
      "source": [
        "### Test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoZ0Ox09EUwe"
      },
      "outputs": [],
      "source": [
        "def merge(path_ori, srcnn, scale):\n",
        "    img_arr = np.array(Image.open(path_ori), dtype=np.uint8)\n",
        "    img_arr = np.array(cv2.cvtColor(np.array(img_arr), cv2.COLOR_RGB2YCrCb), dtype=float)\n",
        "    y_channel = img_arr[:,:,0]\n",
        "    img_ori = tf.expand_dims(crop(y_channel, scale), axis=-1)\n",
        "    h, w, _ = img_ori.shape\n",
        "\n",
        "    down_sample = tf.image.resize(img_ori, (int(h/scale), int(w/scale)), method='bicubic')\n",
        "\n",
        "    img_input = tf.image.resize(down_sample, (h, w), method='bicubic')\n",
        "    patches = []\n",
        "    col = 0\n",
        "    for x in range(0, h-SIZE_INPUT+1, SIZE_LABEL):\n",
        "        row = 0\n",
        "        for y in range(0, w-SIZE_INPUT+1, SIZE_LABEL):\n",
        "            sub_img_input = img_input[x:x+SIZE_INPUT, y:y+SIZE_INPUT]\n",
        "            patches.append(sub_img_input)\n",
        "            row += 1\n",
        "        col += 1\n",
        "    img_in = tf.convert_to_tensor(patches)\n",
        "\n",
        "    res = srcnn.predict(img_in)\n",
        "    img_sr = np.zeros((col*SIZE_LABEL, row*SIZE_LABEL, 1))\n",
        "    print(img_sr.shape)\n",
        "    for i in range(len(res)):\n",
        "        r = i % row\n",
        "        c = i // row\n",
        "        img_sr[c*SIZE_LABEL:(c+1)*SIZE_LABEL, r*SIZE_LABEL:(r+1)*SIZE_LABEL, :] = res[i]\n",
        "\n",
        "    return img_ori, img_input, img_sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79MVcaWKEUwe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}